{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T2Jde8BX1xrd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
        "\"\"\"\n",
        "dl_data = data.split()\n",
        "\n"
      ],
      "metadata": {
        "id": "zTG6hqbY6J6z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2id = tokenizer.word_index\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "metadata": {
        "id": "m2bGtnda6Vox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c61fd0-5ba6-4aea-b6eb-e7b4a5fff95c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 75\n",
            "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size * 2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
        "            label_word.append(word)\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n"
      ],
      "metadata": {
        "id": "nkeeiFX26XD0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size * 2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
      ],
      "metadata": {
        "id": "YHIh5MWiCJ9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20b0a68-60e5-4c21-d245-cfd95910f233"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0.0\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print('Epoch:', epoch, 'Loss:', loss)\n"
      ],
      "metadata": {
        "id": "TF4SHY9tCLvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dc60dc-f7dd-4beb-8fa9-f1aa203a7a43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f5b3e94e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7f5b3e94e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 429.87794494628906\n",
            "Epoch: 2 Loss: 429.8935775756836\n",
            "Epoch: 3 Loss: 428.0065197944641\n",
            "Epoch: 4 Loss: 426.25679111480713\n",
            "Epoch: 5 Loss: 424.8427438735962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = cbow.get_layer('embedding').get_weights()[0]\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1:6] + 1]\n",
        "                 for search_term in ['deep']}\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SgVLe6dCjWy",
        "outputId": "f777ecf2-8f60-4a88-8c89-483486c48d18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'deep': ['convolutional', 'recognition', 'image', 'a', 'they']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBIPv--uwqJk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4TVFpFgG_dk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1f8V8r9HEGr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWI86qIOHGvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}